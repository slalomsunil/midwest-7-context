# TDD Enforcement Checklist for AI Agents

## CRITICAL RULE
**NEVER write implementation code before writing failing tests - NO EXCEPTIONS**

## Pre-Implementation Checklist

Before writing ANY implementation code, ask yourself:

- [ ] Have I written a test that describes the expected behavior?
- [ ] Have I run the test to confirm it fails?
- [ ] Have I seen the specific failure message that confirms what I need to implement?

## TDD Workflow - MANDATORY ORDER

### Phase 1: RED (Write Failing Test)
1. **Create test file** (if it doesn't exist)
2. **Write test describing desired behavior**
   - Use descriptive test names
   - Test ONE specific behavior
   - Include edge cases
3. **Run the test**
   - Confirm it fails
   - Read the failure message
   - Verify it fails for the RIGHT reason

### Phase 2: GREEN (Make Test Pass)
4. **Write minimal implementation**
   - Only write code to make THIS test pass
   - Don't add extra features
   - Keep it simple
5. **Run the test again**
   - Confirm it passes
   - Check for unintended side effects
6. **Run ALL tests**
   - Ensure no regressions
   - Fix any broken tests immediately

### Phase 3: REFACTOR (Improve Code)
7. **Clean up implementation**
   - Remove duplication
   - Improve names
   - Optimize if needed
8. **Run ALL tests again**
   - Ensure refactoring didn't break anything
   - All tests should still pass

## When Changing Existing Features

### Scenario: User Requests Feature Change
**Example**: "Change from badge to row highlighting"

#### âŒ WRONG Approach (What I Did)
```
1. Modify LoggedInUsersPanel.js
2. Update CSS
3. Update tests to match new implementation
```

#### âœ… CORRECT TDD Approach
```
1. Write NEW test: "should apply notification class to user row"
2. Run test â†’ See it FAIL
3. Modify LoggedInUsersPanel.js to add getUserItemClass()
4. Run test â†’ See it PASS
5. Write test: "should apply blinking class when isBlinking is true"
6. Run test â†’ See it FAIL  
7. Update getUserItemClass() to handle blinking
8. Run test â†’ See it PASS
9. Update CSS (no tests needed for styles, but verify visually)
10. Run ALL tests â†’ All should pass
11. Remove old badge-related tests
12. Run ALL tests â†’ All should still pass
```

## Red Flags - When You're Breaking TDD

### ğŸš¨ STOP if you catch yourself:
- Writing implementation before tests
- Saying "I'll write tests after this works"
- Running the app to "see if it works" instead of running tests
- Modifying test expectations to match implementation
- Skipping test run between changes
- Adding multiple features in one test cycle

### âœ… Good Signs You're Doing TDD:
- You see red (failing) tests before green
- You write one small test at a time
- Each test focuses on ONE behavior
- You run tests frequently (after every small change)
- Tests drive your implementation decisions
- You're comfortable with small, incremental changes

## TDD for Different Scenarios

### Adding New Feature
```
1. Test: Feature doesn't exist â†’ FAIL
2. Code: Add minimal feature â†’ PASS
3. Test: Edge case â†’ FAIL
4. Code: Handle edge case â†’ PASS
5. Refactor: Clean up
6. Test: All pass
```

### Fixing Bug
```
1. Test: Reproduce bug â†’ FAIL (proves bug exists)
2. Code: Fix bug â†’ PASS
3. Test: Related scenarios â†’ All pass
4. Code: No further changes needed
```

### Refactoring
```
1. Test: All existing tests â†’ PASS (baseline)
2. Code: Refactor implementation
3. Test: All existing tests â†’ PASS (prove behavior unchanged)
```

### Changing Behavior (like your request)
```
1. Test: New behavior â†’ FAIL
2. Code: Implement new behavior â†’ PASS
3. Test: Old behavior should be removed â†’ Update/remove old tests
4. Test: All tests â†’ PASS
5. Code: Remove old implementation code
6. Test: All tests â†’ STILL PASS
```

## Quick Decision Tree

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ User asks for feature/change    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Do tests exist for current      â”‚
â”‚ behavior?                       â”‚
â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
     â”‚ NO                 â”‚ YES
     â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Write test  â”‚    â”‚ Write test for   â”‚
â”‚ for NEW     â”‚    â”‚ NEW behavior     â”‚
â”‚ behavior    â”‚    â”‚ (should fail)    â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Run test        â”‚
       â”‚ (should FAIL)   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Implement code  â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Run test        â”‚
       â”‚ (should PASS)   â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Run ALL tests   â”‚
       â”‚ Fix failures    â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Clean up old    â”‚
       â”‚ tests/code      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â–¼
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚ Run ALL tests   â”‚
       â”‚ (all pass)      â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Self-Audit Questions

After completing work, ask:

1. **Did I write tests before implementation?**
   - If NO: Go back and retrofit tests, then re-implement

2. **Did I see each test fail before making it pass?**
   - If NO: Delete implementation, run test to see failure, then re-implement

3. **Did I run tests after each small change?**
   - If NO: Run full suite now, fix any issues

4. **Are all tests passing?**
   - If NO: Fix implementation or tests until all pass

5. **Did I remove obsolete tests?**
   - If NO: Clean up test suite

## TDD Commitment Statement

**I commit to:**
- âœ… Write tests BEFORE implementation code
- âœ… Run tests to see failures BEFORE writing code
- âœ… Write minimal code to make tests pass
- âœ… Run ALL tests frequently
- âœ… Refactor only after tests pass
- âœ… Never skip TDD "to save time"
- âœ… Treat RED â†’ GREEN â†’ REFACTOR as sacred

**If I break this commitment:**
- ğŸ”„ Stop immediately
- ğŸ”„ Delete untested implementation
- ğŸ”„ Write the test first
- ğŸ”„ Follow proper TDD cycle

## Integration with Copilot Instructions

This checklist should be referenced BEFORE starting any implementation work, especially when:
- User requests new features
- User reports bugs
- User requests behavior changes
- Refactoring existing code

**Remember: TDD is not optional. It's the MANDATORY development process for this project.**
